{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicholas Paisley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def linear(x,a,b): #creating a definition named \"linear\" with \"x\" , \"a\" , and \"b\" variables\n",
    "    return a*x+b ##returns the equation y(x) = m*x+b or in this case y(x) = a*x+b\n",
    "\n",
    "def loss(x,y_obs,a,b): #creating a definition named \"loss\" with \"x\" , \"y_obs\" , \"a\" and \"b\" variables\n",
    "    y_model = linear(x,a,b) #creating a variable \"y_model\" that returns the linear definition (the slope formula)\n",
    "    return np.sum( (y_model-y_obs)**2 ) #This is the SSR function that is returned in the \"loss\" definition\n",
    "\n",
    "x = np.array( [0,1,2,3,4,5,6,7,8,9,10] ) #Array of numbers for the x coordinates (in an array)\n",
    "y_obs = np.array( [0,1.1,1.9,3.2,3.8,5.1,6.3,6.9,8.5,8.5,10.2] ) #Array of numbers for the y_obs coordinates (in an array)\n",
    "\n",
    "a,b = 2.0, 1.0 #Creating stagnant \"a\" and \"b\" variables (guesses)\n",
    "y_model = linear(x,a,b) #Creating a \"y_model\" variable using the linear defintion with the already defined \"x\" , \"a\" , and \"b\" variables\n",
    "\n",
    "print('First guess at a and b are ',a,b) #prints the a and b variables\n",
    "print('First loss function is ',loss(x,y_obs,a,b)) #prints the SSR function using the defined \"a\", \"b\" and the \"x\" and \"y_obs\" arrays\n",
    "\n",
    "d_by_da = grad(loss,2) #create the derivative/gradient function of \"loss\" --> called d_by_da, (equation(loss), index=2 (a))\n",
    "d_by_db = grad(loss,3) #create the derivative/gradient function of \"loss\" --> called d_by_da, (equation(loss), index=3 (b))\n",
    "\n",
    "\n",
    "#Learning rate gives the rate of speed where the gradient moves during gradient descent. Setting it too high would make your path instable, \n",
    "#too low would make convergence slow. Put it to zero means your model isn't learning anything from the gradients.\n",
    "learning_rate = 0.0001 #controls the magnitude of the vector update (positive number that moves the starting point by a very small number)\n",
    "maximum_number_of_iterations = 1000 #number of iterations given\n",
    "\n",
    "ssr = [] #Sum of Square Residuals -> The lower it is the better it fits the linear model\n",
    "\n",
    "#Ireratively updates according to the learning rate and the value(s) of the gradient\n",
    "\n",
    "for iter in range(maximum_number_of_iterations): \n",
    "    #We are finding new \"a\" (slope) and \"b\" (intercept) variables for 1000 iterations. \n",
    "    a -= learning_rate*d_by_da(x,y_obs,a,b) #(-=) subtracts the value of the expression on the right-hand side from the value on the left-hand side and then assigns the result to the left hand side variable. \n",
    "    #We are changing the slope for every iteration and saving the new slope off as the new \"a\".\n",
    "    b -= learning_rate*d_by_db(x,y_obs,a,b) #(-=) subtracts the value of the expression on the right-hand side from the value on the left-hand side and then assigns the result to the left hand side variable. \n",
    "    #We are changing the intercept for every iteration and saving the new slope off as the new \"b\".\n",
    "    y_model = linear(x,a,b) #Creates our y_model with our \"a\" and \"b\" and the array of \"x\" values\n",
    "    ssr.append(loss(x,y_obs,a,b)) #adds to the empty array list above of all of the SSR generated from the current \"a\" and \"b\".\n",
    "    \n",
    "print('Best a and b are ',a,b) #outputs the \"a\" and \"b\" variable that gives the SSR value\n",
    "print('Best loss function is ',loss(x,y_obs,a,b)) #outputs the best SSR value\n",
    "\n",
    "plt.subplot(1,2,1) #creates 2 plots with this plot being the 1st one shown\n",
    "plt.scatter(x,y_obs) #puts the \"x\" and \"y_obs\" values from the arrays on the graphs\n",
    "plt.plot(x,y_model) #plots the best y_model for the linear regression\n",
    "\n",
    "plt.subplot(1,2,2) #creates 2 plots with this plot being the 1st one shown\n",
    "plt.plot(ssr) #creates a graph of all the calculated SSR values\n",
    "\n",
    "plt.show() #outputs the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
